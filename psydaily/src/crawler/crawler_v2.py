#!/usr/bin/env python3
"""
PsyDaily - å¿ƒç†å­¦æœŸåˆŠçˆ¬è™«
ä¸­è‹±æ–‡æ··åˆæ•°æ®æº
"""

import feedparser
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import os
import time

class JournalCrawler:
    """å¿ƒç†å­¦æœŸåˆŠçˆ¬è™« - ä¸­è‹±æ–‡æ··åˆ"""
    
    # ä¸­è‹±æ–‡æœŸåˆŠRSSæº
    SOURCES = {
        # ä¸­æ–‡æœŸåˆŠ
        'å¿ƒç†å­¦æŠ¥': {
            'url': 'https://journal.psych.ac.cn/rss',  # éœ€è¦éªŒè¯çœŸå®URL
            'type': 'rss',
            'language': 'zh',
            'field': 'general'
        },
        'å¿ƒç†ç§‘å­¦è¿›å±•': {
            'url': 'https://journal.psych.ac.cn/progress/rss',
            'type': 'rss',
            'language': 'zh',
            'field': 'general'
        },
        
        # è‹±æ–‡æœŸåˆŠ - ä½¿ç”¨å¯é çš„RSSæº
        'Nature Human Behaviour': {
            'url': 'https://www.nature.com/nathumbehav.rss',
            'type': 'rss',
            'language': 'en',
            'field': 'neuroscience',
            'impact_factor': 29.9
        },
        'Psychological Science': {
            'url': 'https://journals.sagepub.com/action/showFeed?ui=0&mi=ehikzz&ai=2b4&jc=jpss&type=etoc&feed=rss',
            'type': 'rss',
            'language': 'en',
            'field': 'psychology',
            'impact_factor': 8.2
        },
        'Journal of Experimental Psychology': {
            'url': 'https://psycnet.apa.org/journals/xge.rss',
            'type': 'rss', 
            'language': 'en',
            'field': 'experimental',
            'impact_factor': 5.6
        }
    }
    
    def __init__(self, data_dir='data'):
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
        self.headers = {
            'User-Agent': 'PsyDailyBot/1.0 (Academic Research Purpose)'
        }
    
    def fetch_rss(self, source_name):
        """æŠ“å–RSS feed"""
        source = self.SOURCES.get(source_name)
        if not source:
            return []
        
        try:
            print(f"  æ­£åœ¨æŠ“å–: {source_name}...")
            feed = feedparser.parse(source['url'])
            
            if feed.bozo:
                print(f"  âš ï¸ è­¦å‘Š: {source_name} RSSæ ¼å¼å¯èƒ½æœ‰é—®é¢˜")
            
            articles = []
            for entry in feed.entries[:3]:  # å–æœ€æ–°3ç¯‡
                article = {
                    'title': entry.get('title', '').strip(),
                    'abstract': self._extract_abstract(entry),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', entry.get('updated', '')),
                    'source': source_name,
                    'language': source['language'],
                    'field': source.get('field', 'general'),
                    'impact_factor': source.get('impact_factor', 0),
                    'crawled_at': datetime.now().isoformat()
                }
                articles.append(article)
            
            print(f"  âœ“ {source_name}: {len(articles)}ç¯‡")
            return articles
            
        except Exception as e:
            print(f"  âœ— {source_name} æŠ“å–å¤±è´¥: {e}")
            return []
    
    def _extract_abstract(self, entry):
        """æå–æ‘˜è¦"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µ
        abstract = entry.get('summary', '')
        if not abstract:
            abstract = entry.get('description', '')
        if not abstract and 'content' in entry:
            abstract = entry.content[0].value if entry.content else ''
        
        # æ¸…ç†HTMLæ ‡ç­¾
        if abstract:
            soup = BeautifulSoup(abstract, 'html.parser')
            abstract = soup.get_text(separator=' ').strip()
            # é™åˆ¶é•¿åº¦
            if len(abstract) > 500:
                abstract = abstract[:500] + '...'
        
        return abstract
    
    def fetch_all(self, sources=None):
        """æŠ“å–æ‰€æœ‰æœŸåˆŠ"""
        if sources is None:
            sources = list(self.SOURCES.keys())
        
        print(f"\nğŸ“¥ å¼€å§‹æŠ“å– {len(sources)} ä¸ªæœŸåˆŠ...")
        print("-" * 50)
        
        all_articles = []
        for source_name in sources:
            articles = self.fetch_rss(source_name)
            all_articles.extend(articles)
            time.sleep(1)  # ç¤¼è²Œæ€§å»¶è¿Ÿ
        
        print("-" * 50)
        print(f"âœ“ æ€»è®¡: {len(all_articles)} ç¯‡æ–‡ç« ")
        
        # ä¿å­˜
        if all_articles:
            self.save_articles(all_articles)
        
        return all_articles
    
    def save_articles(self, articles):
        """ä¿å­˜æ–‡ç« åˆ°JSON"""
        filename = f"{self.data_dir}/articles_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜: {filename}")


def demo():
    """æ¼”ç¤º"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     PsyDaily æœŸåˆŠçˆ¬è™«æµ‹è¯•               â•‘
â•‘     ä¸­è‹±æ–‡æ··åˆæ•°æ®æº                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    crawler = JournalCrawler()
    
    # æµ‹è¯•æŠ“å–
    test_sources = ['Nature Human Behaviour', 'å¿ƒç†ç§‘å­¦è¿›å±•']
    articles = crawler.fetch_all(test_sources)
    
    if articles:
        print("\nğŸ“„ æ–‡ç« ç¤ºä¾‹:")
        for i, article in enumerate(articles[:2], 1):
            print(f"\n  {i}. {article['title'][:50]}...")
            print(f"     æ¥æº: {article['source']} ({article['language']})")
            print(f"     å½±å“å› å­: {article.get('impact_factor', 'N/A')}")


if __name__ == '__main__':
    demo()
